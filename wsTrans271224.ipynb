{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWs3Cd35wWDqbUBmszQIkq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wsGit7/Invest/blob/main/wsTrans271224.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GofhDNeqGhk",
        "outputId": "d8549915-3a22-4390-9326-360a3827f543"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.12.14)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2024.12.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2024.12.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=470e0c10a356e1c0f039b21abf90064f74b6731c2ce8327490950429e9808f09\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.7\n",
            "    Uninstalling httpcore-1.0.7:\n",
            "      Successfully uninstalled httpcore-1.0.7\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.2.3 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.57.4 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.12.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from googletrans import Translator\n",
        "import re\n",
        "\n",
        "# Initialize the translator\n",
        "translator = Translator()\n",
        "\n",
        "# Function to translate sentence by sentence\n",
        "def translate_and_insert(file_name):\n",
        "    if not os.path.isfile(file_name):\n",
        "        print(f\"The file '{file_name}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    with open(file_name, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    translated_lines = []\n",
        "    for line in lines:\n",
        "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', line)  # Split by sentences\n",
        "        for sentence in sentences:\n",
        "            if sentence.strip():\n",
        "                translated_lines.append(sentence + '\\n')\n",
        "                translation = translator.translate(sentence, src='de', dest='en')\n",
        "                translated_lines.append(translation.text + '\\n')\n",
        "\n",
        "    with open(file_name, 'w', encoding='utf-8') as file:\n",
        "        file.writelines(translated_lines)\n",
        "\n",
        "    print(f\"The file '{file_name}' has been updated with translations.\")\n",
        "\n",
        "# Specify the file name\n",
        "file_name = 'kasia.txt'\n",
        "\n",
        "# Translate and insert translations\n",
        "translate_and_insert(file_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QA116TfsaG1",
        "outputId": "18c9ea82-a1bf-45fb-859e-1b40cdda5edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file 'kasia.txt' has been updated with translations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_odd_dots(text):\n",
        "    dot_positions = [m.start() for m in re.finditer(r'\\.', text)]\n",
        "    result = list(text)\n",
        "    for i, pos in enumerate(dot_positions):\n",
        "        if (i + 1) % 2 != 0:\n",
        "            result[pos] = ''\n",
        "    return ''.join(result)\n",
        "\n",
        "file_name = 'kasia.txt'\n",
        "\n",
        "try:\n",
        "    with open(file_name, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    new_content = remove_odd_dots(content)\n",
        "\n",
        "    with open(file_name, 'w', encoding='utf-8') as file:\n",
        "        file.write(new_content)\n",
        "\n",
        "    print(f\"The file '{file_name}' has been updated with the odd-numbered dots removed.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{file_name}' does not exist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY8qYUp0wuCe",
        "outputId": "12b03475-a3d7-4095-aaa2-7ba4b4673751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file 'kasia.txt' has been updated with the odd-numbered dots removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the content from kasia.txt\n",
        "with open('kasia.txt', 'r', encoding='utf-8') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Create the HTML content\n",
        "html_content = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Converted Text</title>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>{content}</h1>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Write the HTML content to marke.html\n",
        "with open('marke.html', 'w', encoding='utf-8') as file:\n",
        "    file.write(html_content)\n",
        "\n",
        "print(\"The text from 'kasia.txt' has been transformed and written to 'marek.html'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojTM-Muk4vJC",
        "outputId": "2c431b78-1e84-4fc1-be4b-b816c122ef36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text from 'kasia.txt' has been transformed and written to 'marek.html'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from googletrans import Translator\n",
        "import re\n",
        "\n",
        "# Initialize the translator\n",
        "translator = Translator()\n",
        "\n",
        "# Function to read, translate, and write HTML\n",
        "def translate_and_create_html(input_file, output_file):\n",
        "    if not os.path.isfile(input_file):\n",
        "        print(f\"The file '{input_file}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', content)  # Split by sentences\n",
        "\n",
        "    translated_sentences = []\n",
        "    for sentence in sentences:\n",
        "        if sentence.strip():\n",
        "            translated_sentences.append(sentence + '\\n')\n",
        "            translation = translator.translate(sentence, src='de', dest='en')\n",
        "            translated_sentences.append(translation.text + '\\n')\n",
        "\n",
        "    html_content = \"<!DOCTYPE html>\\n<html lang='en'>\\n<head>\\n<meta charset='UTF-8'>\\n\"\n",
        "    html_content += \"<meta name='viewport' content='width=device-width, initial-scale=1.0'>\\n\"\n",
        "    html_content += \"<title>Translated Text</title>\\n</head>\\n<body>\\n\"\n",
        "\n",
        "    for i in range(0, len(translated_sentences), 2):\n",
        "        html_content += f\"<h1>{translated_sentences[i]}</h1>\\n\"\n",
        "        html_content += f\"<h2>{translated_sentences[i+1]}</h2>\\n<br>\\n\"\n",
        "\n",
        "    html_content += \"</body>\\n</html>\"\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        file.write(html_content)\n",
        "\n",
        "    print(f\"The file '{input_file}' has been translated and written to '{output_file}'.\")\n",
        "\n",
        "# Specify the input and output file names\n",
        "input_file = 'marta.txt'\n",
        "output_file = 'marian.html'\n",
        "\n",
        "# Translate and create HTML\n",
        "translate_and_create_html(input_file, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqjES8D251mp",
        "outputId": "6a79cbac-cf04-423a-f321-f8d1f290493d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file 'marta.txt' has been translated and written to 'marian.html'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==3.1.0a0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "AGY_otQ0UmxK",
        "outputId": "b07f0038-6c3a-49e4-a339-14bf680df0fa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2024.12.14)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2024.12.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16352 sha256=4481933e31c41e3159916df838c517e765d59fd6a9865a5f06a4257a73c2a49f\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/5d/3c/8477d0af4ca2b8b1308812c09f1930863caeebc762fe265a95\n",
            "Successfully built googletrans\n",
            "Installing collected packages: googletrans\n",
            "  Attempting uninstall: googletrans\n",
            "    Found existing installation: googletrans 4.0.0rc1\n",
            "    Uninstalling googletrans-4.0.0rc1:\n",
            "      Successfully uninstalled googletrans-4.0.0rc1\n",
            "Successfully installed googletrans-3.1.0a0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "googletrans"
                ]
              },
              "id": "079284b1f1ce48db8beec91f57b9625a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_german_text(input_file):\n",
        "    # Define a German alphabet regex pattern including dot (.) and single space\n",
        "    german_alphabet_pattern = re.compile(r\"[^a-zA-ZäöüÄÖÜß. ]\")\n",
        "\n",
        "    # Read the input file\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        content = file.readlines()\n",
        "\n",
        "    cleaned_lines = []\n",
        "    for line in content:\n",
        "        # Remove unwanted characters\n",
        "        cleaned_line = german_alphabet_pattern.sub('', line)\n",
        "        # Add non-empty lines to the cleaned list\n",
        "        if cleaned_line.strip():\n",
        "            cleaned_lines.append(cleaned_line.strip() + '\\n')\n",
        "\n",
        "    # Write the cleaned content back to the file\n",
        "    with open(input_file, 'w', encoding='utf-8') as file:\n",
        "        file.writelines(cleaned_lines)\n",
        "\n",
        "    print(f\"The file '{input_file}' has been cleaned and updated.\")\n",
        "\n",
        "# Specify the input file name\n",
        "input_file = 'marta.txt'\n",
        "\n",
        "# Clean the text\n",
        "clean_german_text(input_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o3Q509jQmD5",
        "outputId": "5376615d-6551-46e5-a9ef-77856ddec258"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file 'marta.txt' has been cleaned and updated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sg5dUM0HOng3",
        "outputId": "e60af22d-6cbc-451e-e10f-032b961c9355"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "def pdf_to_text(pdf_path, txt_path):\n",
        "    # Open the PDF file\n",
        "    pdf_document = fitz.open(pdf_path)\n",
        "\n",
        "    # Open a text file to write the extracted text\n",
        "    with open(txt_path, 'w', encoding='utf-8') as txt_file:\n",
        "        # Iterate through each page\n",
        "        for page_num in range(len(pdf_document)):\n",
        "            page = pdf_document[page_num]\n",
        "            text = page.get_text()\n",
        "            txt_file.write(text)\n",
        "            txt_file.write('\\n')\n",
        "\n",
        "    print(f\"PDF content has been successfully written to {txt_path}\")\n",
        "\n",
        "# Specify the input and output file names\n",
        "pdf_path = 'AEB-2024_gesamt-barrierefrei_mit-Schlussklausel.pdf'\n",
        "txt_path = 'marta.txt'\n",
        "\n",
        "# Convert the PDF to a text file\n",
        "pdf_to_text(pdf_path, txt_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3E_mU6NOwce",
        "outputId": "ac13c0bf-93c7-49bd-fb0c-7290aafc370f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF content has been successfully written to marta.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nowa sekcja"
      ],
      "metadata": {
        "id": "oPcwo3auFzo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_short_lines(input_file):\n",
        "    # Read the input file\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Filter out empty lines or lines with only one character\n",
        "    cleaned_lines = [line for line in lines if len(line.strip()) > 4]\n",
        "\n",
        "    # Write the cleaned content back to the file\n",
        "    with open(input_file, 'w', encoding='utf-8') as file:\n",
        "        file.writelines(cleaned_lines)\n",
        "\n",
        "    print(f\"The file '{input_file}' has been cleaned of empty or single character lines.\")\n",
        "\n",
        "# Specify the input file name\n",
        "input_file = 'marian.html'\n",
        "\n",
        "# Delete short lines\n",
        "delete_short_lines(input_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--7a0TagdOtj",
        "outputId": "565539a1-1555-489f-ab7c-6fd3c7176917"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file 'marian.html' has been cleaned of empty or single character lines.\n"
          ]
        }
      ]
    }
  ]
}