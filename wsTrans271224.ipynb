{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUeW2e/vZbkcx67cbcYmft",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wsGit7/Invest/blob/main/wsTrans271224.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GofhDNeqGhk",
        "outputId": "a6bc837f-7235-47a4-d27c-1b16a0ae26ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.12.14)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2024.12.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2024.12.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=3caebce4d2895cd392d7a8c9b4651b8689699b8fee66762747fb19d76f574aff\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.7\n",
            "    Uninstalling httpcore-1.0.7:\n",
            "      Successfully uninstalled httpcore-1.0.7\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.2.3 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.57.4 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.12.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mEzqN06F4oNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "INSTALL PDF2"
      ],
      "metadata": {
        "id": "AnvNml4i6kCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGyJa8z61dan",
        "outputId": "e03b6aad-a0c0-4330-cf7b-db0edcdc2c44"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PDF 2 TXT"
      ],
      "metadata": {
        "id": "EuS8hzzo4qW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "import re\n",
        "\n",
        "def process_text(text):\n",
        "    lines = text.split('\\n')\n",
        "    processed_lines = []\n",
        "    for line in lines:\n",
        "        words = line.split()\n",
        "        new_line = ''\n",
        "        for word in words:\n",
        "            # Replace decimal dots with commas\n",
        "            if re.match(r'^\\d+\\.\\d+$', word):\n",
        "                new_line += word.replace('.', ',') + ' '\n",
        "            else:\n",
        "                # Remove dots not at the end of a sentence\n",
        "                if word.endswith('.'):\n",
        "                    if not re.match(r'^\\d+(\\.\\d+)*$', word):  # Ensure it's not a number\n",
        "                        new_line += word + ' '\n",
        "                    else:\n",
        "                        new_line += word.replace('.', ',') + ' '\n",
        "                else:\n",
        "                    new_line += word.replace('.', '') + ' '\n",
        "        processed_lines.append(new_line.strip())\n",
        "    return '\\n'.join(processed_lines)\n",
        "\n",
        "def convert_pdf_to_txt(pdf_file, txt_file):\n",
        "    pdf_reader = PdfReader(pdf_file)\n",
        "    text = ''\n",
        "    for page in pdf_reader.pages:\n",
        "        text += page.extract_text() + '\\n'\n",
        "\n",
        "    processed_text = process_text(text)\n",
        "\n",
        "    with open(txt_file, 'w') as file:\n",
        "        file.write(processed_text)\n",
        "\n",
        "# Usage\n",
        "convert_pdf_to_txt('tomek.pdf', 'tomek.txt')\n"
      ],
      "metadata": {
        "id": "UuRU1vBg1-P4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRANSLATE TXT"
      ],
      "metadata": {
        "id": "vh9jX7dw4ucV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from googletrans import Translator\n",
        "import re\n",
        "\n",
        "# Initialize the translator\n",
        "translator = Translator()\n",
        "\n",
        "# Function to translate sentence by sentence\n",
        "def translate_and_insert(file_name):\n",
        "    if not os.path.isfile(file_name):\n",
        "        print(f\"The file '{file_name}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    with open(file_name, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    translated_lines = []\n",
        "    for line in lines:\n",
        "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', line)  # Split by sentences\n",
        "        for sentence in sentences:\n",
        "            if sentence.strip():\n",
        "                translated_lines.append(sentence + '\\n')\n",
        "                translation = translator.translate(sentence, src='de', dest='en')\n",
        "                translated_lines.append(translation.text + '\\n')\n",
        "\n",
        "    with open(file_name, 'w', encoding='utf-8') as file:\n",
        "        file.writelines(translated_lines)\n",
        "\n",
        "    print(f\"The file '{file_name}' has been updated with translations.\")\n",
        "\n",
        "# Specify the file name\n",
        "file_name = 'tomek.txt'\n",
        "\n",
        "# Translate and insert translations\n",
        "translate_and_insert(file_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QA116TfsaG1",
        "outputId": "18c9ea82-a1bf-45fb-859e-1b40cdda5edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file 'kasia.txt' has been updated with translations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_odd_dots(text):\n",
        "    dot_positions = [m.start() for m in re.finditer(r'\\.', text)]\n",
        "    result = list(text)\n",
        "    for i, pos in enumerate(dot_positions):\n",
        "        if (i + 1) % 2 != 0:\n",
        "            result[pos] = ''\n",
        "    return ''.join(result)\n",
        "\n",
        "file_name = 'kasia.txt'\n",
        "\n",
        "try:\n",
        "    with open(file_name, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    new_content = remove_odd_dots(content)\n",
        "\n",
        "    with open(file_name, 'w', encoding='utf-8') as file:\n",
        "        file.write(new_content)\n",
        "\n",
        "    print(f\"The file '{file_name}' has been updated with the odd-numbered dots removed.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{file_name}' does not exist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY8qYUp0wuCe",
        "outputId": "12b03475-a3d7-4095-aaa2-7ba4b4673751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file 'kasia.txt' has been updated with the odd-numbered dots removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the content from kasia.txt\n",
        "with open('kasia.txt', 'r', encoding='utf-8') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Create the HTML content\n",
        "html_content = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Converted Text</title>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>{content}</h1>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Write the HTML content to marke.html\n",
        "with open('marke.html', 'w', encoding='utf-8') as file:\n",
        "    file.write(html_content)\n",
        "\n",
        "print(\"The text from 'kasia.txt' has been transformed and written to 'marek.html'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojTM-Muk4vJC",
        "outputId": "2c431b78-1e84-4fc1-be4b-b816c122ef36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text from 'kasia.txt' has been transformed and written to 'marek.html'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from googletrans import Translator\n",
        "import re\n",
        "\n",
        "# Initialize the translator\n",
        "translator = Translator()\n",
        "\n",
        "# Function to read, translate, and write HTML\n",
        "def translate_and_create_html(input_file, output_file):\n",
        "    if not os.path.isfile(input_file):\n",
        "        print(f\"The file '{input_file}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', content)  # Split by sentences\n",
        "\n",
        "    translated_sentences = []\n",
        "    for sentence in sentences:\n",
        "        if sentence.strip():\n",
        "            translated_sentences.append(sentence + '\\n')\n",
        "            translation = translator.translate(sentence, src='de', dest='en')\n",
        "            translated_sentences.append(translation.text + '\\n')\n",
        "\n",
        "    html_content = \"<!DOCTYPE html>\\n<html lang='en'>\\n<head>\\n<meta charset='UTF-8'>\\n\"\n",
        "    html_content += \"<meta name='viewport' content='width=device-width, initial-scale=1.0'>\\n\"\n",
        "    html_content += \"<title>Translated Text</title>\\n</head>\\n<body>\\n\"\n",
        "\n",
        "    for i in range(0, len(translated_sentences), 2):\n",
        "        html_content += f\"<h1>{translated_sentences[i]}</h1>\\n\"\n",
        "        html_content += f\"<h2>{translated_sentences[i+1]}</h2>\\n<br>\\n\"\n",
        "\n",
        "    html_content += \"</body>\\n</html>\"\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        file.write(html_content)\n",
        "\n",
        "    print(f\"The file '{input_file}' has been translated and written to '{output_file}'.\")\n",
        "\n",
        "# Specify the input and output file names\n",
        "input_file = 'kasia.txt'\n",
        "output_file = 'marek.html'\n",
        "\n",
        "# Translate and create HTML\n",
        "translate_and_create_html(input_file, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqjES8D251mp",
        "outputId": "dae7874a-baaa-4e4d-bdcd-339602f99719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file 'kasia.txt' has been translated and written to 'marek.html'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nowa sekcja"
      ],
      "metadata": {
        "id": "oPcwo3auFzo9"
      }
    }
  ]
}