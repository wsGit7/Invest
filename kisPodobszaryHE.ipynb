{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjrkSNt851ZMLVeNI0BLsP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wsGit7/Invest/blob/main/kisPodobszaryHE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lYKAf9ZTToS",
        "outputId": "ebcb1d74-ecb6-4dc6-96cb-b63358651a7a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install reportlab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7zafd7DThir",
        "outputId": "79df0a18-4d80-416d-9f93-51783a7377b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reportlab\n",
            "  Downloading reportlab-4.4.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from reportlab) (11.2.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from reportlab) (5.2.0)\n",
            "Downloading reportlab-4.4.1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: reportlab\n",
            "Successfully installed reportlab-4.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRsnwNkJPWF1",
        "outputId": "da2f68c9-fbef-4468-a7af-771ff005052c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison completed. Results written to ewa.pdf\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import difflib\n",
        "from PyPDF2 import PdfReader\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas\n",
        "\n",
        "# Parameters:\n",
        "SIMILARITY_THRESHOLD = 0.001       # Only consider paragraphs with similarity ratio of 0.8 or above\n",
        "MIN_SUBSTRING_LENGTH = 6        # Minimal length (in characters) of the common phrase to be noted\n",
        "\n",
        "def extract_paragraphs(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from each page of a PDF and splits the text into paragraphs.\n",
        "    Returns a list of tuples: (page_number, paragraph_index, paragraph_text).\n",
        "    \"\"\"\n",
        "    paragraphs = []\n",
        "    reader = PdfReader(pdf_path)\n",
        "    for page_idx, page in enumerate(reader.pages):\n",
        "        text = page.extract_text() or \"\"\n",
        "        # We split the page text based on newlines.\n",
        "        # Depending on your PDFs, you might want to refine the splitting mechanism.\n",
        "        para_list = [p.strip() for p in text.split(\"\\n\") if p.strip() != \"\"]\n",
        "        for par_idx, para in enumerate(para_list):\n",
        "            paragraphs.append((page_idx + 1, par_idx + 1, para))\n",
        "    return paragraphs\n",
        "\n",
        "def longest_common_substring(s1, s2):\n",
        "    \"\"\"\n",
        "    Uses difflib to retrieve the longest common substring between two strings.\n",
        "    \"\"\"\n",
        "    matcher = difflib.SequenceMatcher(None, s1, s2)\n",
        "    match = matcher.find_longest_match(0, len(s1), 0, len(s2))\n",
        "    return s1[match.a: match.a + match.size]\n",
        "\n",
        "def main():\n",
        "    # Extract paragraphs from asia.pdf.\n",
        "    asia_pdf_path = \"asia.pdf\"\n",
        "    asia_paragraphs = extract_paragraphs(asia_pdf_path)\n",
        "\n",
        "    # List all PDF files in the folder /andrzej.\n",
        "    pdf_files = glob.glob(os.path.join(\"/andrzej\", \"*.pdf\"))\n",
        "\n",
        "    results = []  # This will hold all matching results\n",
        "\n",
        "    # Process each document in /andrzej.\n",
        "    for file_path in pdf_files:\n",
        "        doc_name = os.path.basename(file_path)\n",
        "        doc_paragraphs = extract_paragraphs(file_path)\n",
        "\n",
        "        # Compare every paragraph from asia.pdf with every paragraph from the current document.\n",
        "        for asia_page, asia_par_index, asia_text in asia_paragraphs:\n",
        "            for doc_page, doc_par_index, doc_text in doc_paragraphs:\n",
        "                # Compute a similarity ratio.\n",
        "                similarity = difflib.SequenceMatcher(None, asia_text, doc_text).ratio()\n",
        "                if similarity >= SIMILARITY_THRESHOLD:\n",
        "                    common_phrase = longest_common_substring(asia_text, doc_text)\n",
        "                    # Only record the result if the common phrase is of meaningful length.\n",
        "                    if len(common_phrase) >= MIN_SUBSTRING_LENGTH:\n",
        "                        results.append({\n",
        "                            \"asia_page\": asia_page,\n",
        "                            \"asia_paragraph\": asia_par_index,\n",
        "                            \"doc_name\": doc_name,\n",
        "                            \"doc_page\": doc_page,\n",
        "                            \"doc_paragraph\": doc_par_index,\n",
        "                            \"common_phrase\": common_phrase,\n",
        "                            \"similarity\": similarity\n",
        "                        })\n",
        "\n",
        "    # Write the matching results to ewa.pdf using ReportLab.\n",
        "    output_pdf_path = \"ewa.pdf\"\n",
        "    c = canvas.Canvas(output_pdf_path, pagesize=letter)\n",
        "    width, height = letter\n",
        "    c.setFont(\"Helvetica\", 10)\n",
        "\n",
        "    text_object = c.beginText(40, height - 40)\n",
        "    text_object.textLine(\"Comparison Results Between asia.pdf and PDFs in /andrzej\")\n",
        "    text_object.textLine(\"Similarity Threshold: {:.2f}\".format(SIMILARITY_THRESHOLD))\n",
        "    text_object.textLine(\"\")\n",
        "\n",
        "    # Each result line holds the match details.\n",
        "    for result in results:\n",
        "        line = (\"asia.pdf (Page {}, Paragraph {}) <-> {} (Page {}, Paragraph {}), \"\n",
        "                \"Similarity: {:.2f}, Common Phrase: '{}'\".format(\n",
        "            result[\"asia_page\"],\n",
        "            result[\"asia_paragraph\"],\n",
        "            result[\"doc_name\"],\n",
        "            result[\"doc_page\"],\n",
        "            result[\"doc_paragraph\"],\n",
        "            result[\"similarity\"],\n",
        "            result[\"common_phrase\"]\n",
        "        ))\n",
        "        text_object.textLine(line)\n",
        "\n",
        "        # If we are low on space, start a new page.\n",
        "        if text_object.getY() < 50:\n",
        "            c.drawText(text_object)\n",
        "            c.showPage()\n",
        "            text_object = c.beginText(40, height - 40)\n",
        "            text_object.setFont(\"Helvetica\", 10)\n",
        "\n",
        "    c.drawText(text_object)\n",
        "    c.save()\n",
        "    print(f\"Comparison completed. Results written to {output_pdf_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import re\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text from all pages of a PDF and returns a single concatenated string.\"\"\"\n",
        "    full_text = \"\"\n",
        "    with open(pdf_path, \"rb\") as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        for page in reader.pages:\n",
        "            text = page.extract_text() or \"\"\n",
        "            full_text += text + \" \"\n",
        "    return full_text\n",
        "\n",
        "def get_trigrams(text):\n",
        "    \"\"\"\n",
        "    Given a string of text, returns a set of all three-word sequences.\n",
        "    Words are extracted using regex (considering alphanumeric characters) and normalized to lowercase.\n",
        "    \"\"\"\n",
        "    # Extract words and lower-case them.\n",
        "    words = re.findall(r'\\w+', text.lower())\n",
        "    # Generate three-word sequences (trigrams)\n",
        "    trigrams = {\" \".join(words[i:i+3]) for i in range(len(words) - 2)}\n",
        "    return trigrams\n",
        "\n",
        "def main():\n",
        "    # Extract text from a.pdf and b.pdf in the current directory.\n",
        "    text_a = extract_text_from_pdf(\"a.pdf\")\n",
        "    text_b = extract_text_from_pdf(\"b.pdf\")\n",
        "\n",
        "    # Generate trigrams for both documents.\n",
        "    trigrams_a = get_trigrams(text_a)\n",
        "    trigrams_b = get_trigrams(text_b)\n",
        "\n",
        "    # Compute the common three-word sequences.\n",
        "    common_trigrams = trigrams_a.intersection(trigrams_b)\n",
        "\n",
        "    if common_trigrams:\n",
        "        print(f\"Found {len(common_trigrams)} common three-word sequences from a.pdf present in b.pdf:\")\n",
        "        for trigram in common_trigrams:\n",
        "            print(trigram)\n",
        "    else:\n",
        "        print(\"No matching three-word sequences found.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUAmidnVTROT",
        "outputId": "45330c8a-243a-4be8-d3a7-7732f76158cc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 203 common three-word sequences from a.pdf present in b.pdf:\n",
            "products such as\n",
            "to provide the\n",
            "in the field\n",
            "for the implementation\n",
            "systems and communication\n",
            "the work of\n",
            "life cycle of\n",
            "reduction of the\n",
            "do not meet\n",
            "the scope of\n",
            "take into account\n",
            "the availability of\n",
            "which they are\n",
            "elements of the\n",
            "order to achieve\n",
            "that do not\n",
            "the needs of\n",
            "necessary for the\n",
            "improve the efficiency\n",
            "be considered as\n",
            "legal and administrative\n",
            "of applications and\n",
            "the development of\n",
            "and exploitation of\n",
            "of high quality\n",
            "accordance with the\n",
            "taking into account\n",
            "the context of\n",
            "the participation of\n",
            "depending on the\n",
            "in the eu\n",
            "facilities and or\n",
            "early detection of\n",
            "research areas in\n",
            "and effectiveness of\n",
            "in the work\n",
            "in the early\n",
            "learning algorithms and\n",
            "to the development\n",
            "to monitor and\n",
            "in line with\n",
            "aspects of the\n",
            "of the full\n",
            "for the purposes\n",
            "taken into account\n",
            "for the detection\n",
            "the quality of\n",
            "carrying out the\n",
            "one of the\n",
            "needs of the\n",
            "it is the\n",
            "to achieve the\n",
            "the eu s\n",
            "use of materials\n",
            "the costs of\n",
            "the research and\n",
            "access to the\n",
            "and the quality\n",
            "availability of the\n",
            "the efficiency and\n",
            "the amount of\n",
            "the state of\n",
            "in the treatment\n",
            "strategy of the\n",
            "the purpose of\n",
            "for the provision\n",
            "and or the\n",
            "the cost of\n",
            "and procedures for\n",
            "part of the\n",
            "respond to the\n",
            "of the proposed\n",
            "for the monitoring\n",
            "interaction with the\n",
            "in the next\n",
            "necessary to develop\n",
            "the ability to\n",
            "of the actual\n",
            "throughout the life\n",
            "support for the\n",
            "be taken into\n",
            "phase of the\n",
            "for the purpose\n",
            "technologies and tools\n",
            "the field of\n",
            "of the new\n",
            "the identification of\n",
            "impact of the\n",
            "in particular it\n",
            "and for the\n",
            "0 to 5\n",
            "regardless of the\n",
            "is important to\n",
            "the assessment of\n",
            "particular emphasis on\n",
            "it is also\n",
            "to carry out\n",
            "of both the\n",
            "machine learning algorithms\n",
            "the duration of\n",
            "based on the\n",
            "which will be\n",
            "the collection and\n",
            "physical and mental\n",
            "development of novel\n",
            "it will be\n",
            "state of the\n",
            "the purposes of\n",
            "models for the\n",
            "stages of the\n",
            "implementation of the\n",
            "well as other\n",
            "carry out a\n",
            "the need to\n",
            "as well as\n",
            "all stages of\n",
            "to redefine the\n",
            "the detection of\n",
            "to ensure the\n",
            "of the following\n",
            "for which the\n",
            "in the following\n",
            "plant and animal\n",
            "development of a\n",
            "conditions for the\n",
            "for the dissemination\n",
            "for research and\n",
            "basis of the\n",
            "into account the\n",
            "relation to the\n",
            "and quality control\n",
            "for this purpose\n",
            "the transition from\n",
            "should be taken\n",
            "or b the\n",
            "and can be\n",
            "the time of\n",
            "and the need\n",
            "new materials and\n",
            "that will be\n",
            "in relation to\n",
            "as part of\n",
            "monitoring and the\n",
            "the functioning of\n",
            "a structure of\n",
            "a high level\n",
            "study of the\n",
            "innovative methods and\n",
            "the life cycle\n",
            "according to the\n",
            "to respond to\n",
            "in terms of\n",
            "related to the\n",
            "in medicine and\n",
            "the implementation of\n",
            "implementation in the\n",
            "of information in\n",
            "at least one\n",
            "they will be\n",
            "used for the\n",
            "in the context\n",
            "due to the\n",
            "the same time\n",
            "and efficiency of\n",
            "and tools for\n",
            "and data related\n",
            "line with the\n",
            "in order to\n",
            "the treatment of\n",
            "at all stages\n",
            "will be used\n",
            "the evaluation of\n",
            "or in the\n",
            "at the same\n",
            "the protection of\n",
            "including in particular\n",
            "to improve the\n",
            "the conditions of\n",
            "the preparation of\n",
            "all areas of\n",
            "same time the\n",
            "evaluation of the\n",
            "in accordance with\n",
            "in the implementation\n",
            "emphasis on the\n",
            "efficiency of the\n",
            "each of the\n",
            "context of the\n",
            "purposes of the\n",
            "and with the\n",
            "it is important\n",
            "purpose of the\n",
            "refers to the\n",
            "the level of\n",
            "which can be\n",
            "of the real\n",
            "in the world\n",
            "the basis of\n",
            "materials and new\n",
            "and assessing the\n",
            "for the identification\n",
            "it tools to\n",
            "the most effective\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import PyPDF2\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas\n",
        "\n",
        "def extract_paragraphs(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from each page of a PDF file and splits the text into paragraphs\n",
        "    (using newlines). Returns a list of tuples: (page_number, paragraph_number, paragraph_text).\n",
        "    \"\"\"\n",
        "    paragraphs = []\n",
        "    with open(pdf_path, \"rb\") as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        for page_num, page in enumerate(reader.pages, start=1):\n",
        "            text = page.extract_text() or \"\"\n",
        "            # Split text on newlines (adjust splitting if necessary)\n",
        "            para_list = [p.strip() for p in text.split(\"\\n\") if p.strip() != \"\"]\n",
        "            for para_idx, para in enumerate(para_list, start=1):\n",
        "                paragraphs.append((page_num, para_idx, para))\n",
        "    return paragraphs\n",
        "\n",
        "def get_trigrams_from_text(text):\n",
        "    \"\"\"\n",
        "    Given a paragraph text, returns a list of trigrams (three consecutive words).\n",
        "    The text is normalized to lowercase and split using regex to capture words.\n",
        "    \"\"\"\n",
        "    words = re.findall(r'\\w+', text.lower())\n",
        "    trigrams = [\" \".join(words[i:i+3]) for i in range(len(words) - 2)]\n",
        "    return trigrams\n",
        "\n",
        "def build_trigram_map(paragraphs):\n",
        "    \"\"\"\n",
        "    For each paragraph in the list, compute its trigrams and store them in a dictionary.\n",
        "    The dictionary's keys are the trigrams and the value is a list of occurrences,\n",
        "    where an occurrence is a tuple (page_number, paragraph_number).\n",
        "    \"\"\"\n",
        "    trigram_map = {}\n",
        "    for page, para_idx, text in paragraphs:\n",
        "        trigrams = get_trigrams_from_text(text)\n",
        "        for trigram in trigrams:\n",
        "            if trigram not in trigram_map:\n",
        "                trigram_map[trigram] = []\n",
        "            trigram_map[trigram].append((page, para_idx))\n",
        "    return trigram_map\n",
        "\n",
        "def compare_trigrams(map_a, map_b):\n",
        "    \"\"\"\n",
        "    Finds common trigrams between two dictionaries. For each common trigram,\n",
        "    creates a record for every occurrence combination between a.pdf and b.pdf.\n",
        "    Returns a list of tuples:\n",
        "      (common_trigram, (page_a, para_a), (page_b, para_b))\n",
        "    \"\"\"\n",
        "    common_matches = []\n",
        "    common_keys = set(map_a.keys()).intersection(set(map_b.keys()))\n",
        "    for trigram in common_keys:\n",
        "        occurrences_a = map_a[trigram]\n",
        "        occurrences_b = map_b[trigram]\n",
        "        for occ_a in occurrences_a:\n",
        "            for occ_b in occurrences_b:\n",
        "                common_matches.append((trigram, occ_a, occ_b))\n",
        "    return common_matches\n",
        "\n",
        "def write_results_to_pdf(output_path, matches):\n",
        "    \"\"\"\n",
        "    Writes the results (each match with trigram, page and paragraph numbers in a.pdf and b.pdf)\n",
        "    to an output PDF (using ReportLab).\n",
        "    \"\"\"\n",
        "    c = canvas.Canvas(output_path, pagesize=letter)\n",
        "    width, height = letter\n",
        "    c.setFont(\"Helvetica\", 10)\n",
        "    y = height - 50    # Starting y position\n",
        "    line_height = 12\n",
        "\n",
        "    header = \"Comparison Results between a.pdf and b.pdf\\n\" \\\n",
        "             \"Matching three-word sequences with their page and paragraph positions:\\n\"\n",
        "    c.drawString(50, y, header)\n",
        "    y -= 30\n",
        "\n",
        "    for match in matches:\n",
        "        trigram, pos_a, pos_b = match\n",
        "        line = (f\"Trigram: '{trigram}' | \"\n",
        "                f\"a.pdf - Page {pos_a[0]}, Paragraph {pos_a[1]} | \"\n",
        "                f\"b.pdf - Page {pos_b[0]}, Paragraph {pos_b[1]}\")\n",
        "        c.drawString(50, y, line)\n",
        "        y -= line_height\n",
        "\n",
        "        # Check if we've reached near the bottom of the page\n",
        "        if y < 50:\n",
        "            c.showPage()\n",
        "            y = height - 50\n",
        "            c.setFont(\"Helvetica\", 10)\n",
        "\n",
        "    c.save()\n",
        "\n",
        "def main():\n",
        "    # Extract paragraphs for each PDF file.\n",
        "    paragraphs_a = extract_paragraphs(\"a.pdf\")\n",
        "    paragraphs_b = extract_paragraphs(\"b.pdf\")\n",
        "\n",
        "    # Build trigram mappings from paragraphs.\n",
        "    trigram_map_a = build_trigram_map(paragraphs_a)\n",
        "    trigram_map_b = build_trigram_map(paragraphs_b)\n",
        "\n",
        "    # Compare the two maps to get matching three-word sequences.\n",
        "    matches = compare_trigrams(trigram_map_a, trigram_map_b)\n",
        "\n",
        "    # Write the results into 'apple.pdf'.\n",
        "    output_pdf_path = \"apple.pdf\"\n",
        "    write_results_to_pdf(output_pdf_path, matches)\n",
        "\n",
        "    print(f\"Comparison completed. Found {len(matches)} matches. Results saved to {output_pdf_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvN8Tm54YHkZ",
        "outputId": "c8376561-98c4-4e63-d9d3-fed4f2191065"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison completed. Found 1491 matches. Results saved to apple.pdf\n"
          ]
        }
      ]
    }
  ]
}